{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 10.198116261874107\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n",
      "Error 11.520360864672174\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import matrix as M\n",
    "import time\n",
    "\n",
    "def relu(x, d=0):\n",
    "    #d==True during backprop\n",
    "    x[x < 0] = 0\n",
    "    if d: x = np.where(x <= 0, 1, 0)\n",
    "    return x\n",
    "\n",
    "def sigma(x, d=0):\n",
    "    x = np.divide(1, (1 + np.exp(-x))) \n",
    "    if d: x = x - (1 - x)\n",
    "    return x\n",
    "\n",
    "def softmax(x, d=0):\n",
    "    #d==True during backprop\n",
    "    x = np.divide(np.exp(x), sum(np.exp(x))) #if not d else x.T.dot(1-x) #OBS solve where i != j\n",
    "    return x\n",
    "\n",
    "def KL_divergence(X, Y, d=0):\n",
    "    #d==True during backprop\n",
    "    #\n",
    "    #Below derived formula obtained from http://proceedings.mlr.press/v37/theis15-supp.pdf\n",
    "    #The derivative of the KL divergence with respect to the output is np.dot((X-Y).T, np.identity(X.shape[0]))\n",
    "    #\n",
    "    #Below derived formula obtained from: https://deepnotes.io/softmax-crossentropy\n",
    "    #the derivative of the loss with respect to the softmax-crossentropy is simply Y-X \n",
    "    #(so the following derivative only works if the last layer is a softmax layer)\n",
    "    loss = - Y.dot(np.log(X)) if not d else np.dot((X-Y).T, np.identity(X.shape[0])) #X - Y\n",
    "    return loss\n",
    "\n",
    "# Shape(3, 3)\n",
    "# 3 input neurons and one layer with 10 hidden neurons and an output of three neurons\n",
    "\n",
    "def rnn(shape, activation):\n",
    "    # [3, 10, 3 ,1]\n",
    "    # return layer_stack\n",
    "    pass\n",
    "\n",
    "def initialize_layer(previous_layer_neurons, this_layer_neurons,\n",
    "                     activation_function=relu): \n",
    "    \n",
    "    size = (previous_layer_neurons, this_layer_neurons)\n",
    "    weights = np.random.standard_normal(size)\n",
    "    # http://cs231n.github.io/neural-networks-2/ initialized at 0.001\n",
    "    biases = np.full(this_layer_neurons, 0.001)\n",
    "    previous_activation = None #ready for input size(previous layer neuron)\n",
    "    \n",
    "    layer = dict([(\"W\", weights), (\"b\", biases), (\"f\", activation_function), \n",
    "                  (\"a\", previous_activation)])\n",
    "    return layer\n",
    "\n",
    "def activate_layer(layer, d=0):\n",
    "    #this_layer_weights\n",
    "    W = layer[\"W\"]\n",
    "    #previous_layer_output\n",
    "    a = layer[\"a\"]\n",
    "    #bias\n",
    "    b = layer[\"b\"]\n",
    "    #activation_function\n",
    "    f = layer[\"f\"]\n",
    "    #logits aka. pre-nonlinearity activations\n",
    "    #print(\"b-\" if d else \"f-\", \"a\", a.shape,\"W\",W.shape,\"b\", b.shape)\n",
    "    z = a.dot(W) + b #if not d else a.dot(W.T) #OBS! needs + bias_t-1\n",
    "    #print(z.shape)\n",
    "    return f(z, d=d)\n",
    "\n",
    "def add_layer(layer, L=[]):\n",
    "    #a layer must be a dict of weigts, bias, activation function, \n",
    "    #and a tensor of the previous layers activation output\n",
    "    return L + [layer]\n",
    "\n",
    "def feed_input_to_stack(input_data, layer_stack):\n",
    "    #set previous_layer_activation of first layer to be the input\n",
    "    layer_stack[0][\"a\"] = input_data\n",
    "    return layer_stack\n",
    "\n",
    "def forward_pass(layer_stack, train_data):\n",
    "    #previous_activation output\n",
    "    out = train_data\n",
    "    for i, layer in enumerate(layer_stack):\n",
    "        layer_stack[i][\"a\"] = out\n",
    "        out = activate_layer(layer)\n",
    "        #print(\"f\",i)\n",
    "    return out, layer_stack\n",
    "\n",
    "def back_prop(layer_stack, dloss):\n",
    "    #loss of output layer\n",
    "    g = dloss\n",
    "    layer_out = layer_stack[-1]\n",
    "    W = layer_out[\"W\"]\n",
    "    \n",
    "    dzy_out = activate_layer(layer_out, d=True) #dy/dz\n",
    "    g = np.multiply(g, dzy_out)\n",
    "    \n",
    "    dWz_out = layer_out[\"a\"] #dzt/dWt = at-1\n",
    "    dW_out = M(dWz_out).T.dot(M(g))    \n",
    "    g = W.dot(g.T)\n",
    "    \n",
    "    #stochastic gradient update\n",
    "    layer_stack[-1][\"W\"] = np.multiply(W , dW_out *1.1 )\n",
    "    \n",
    "    for i, layer in enumerate(reversed(layer_stack[:-1])):\n",
    "        #activate_layer() recomputes z to minimize lines of code (and save some memory)\n",
    "        # It is then activated with the derived activation function\n",
    "        da = activate_layer(layer, d=True)\n",
    "        W = layer[\"W\"]\n",
    "        b = layer[\"b\"]\n",
    "        #print()\n",
    "        dWz = layer[\"a\"]\n",
    "        #print(\"a\", dWz.shape)\n",
    "        g = np.multiply(g, da)\n",
    "        #print(\"g\", g.shape)\n",
    "        dW = np.array(M(dWz).T.dot(M(g)))\n",
    "        #print(\"dW\", dW.shape)\n",
    "        #print()\n",
    "        g = W.dot(g.T)\n",
    "        #print(\"b\", i)\n",
    "        #\n",
    "        #layer_stack[i][\"b\"] = np.subtract(b , g)\n",
    "        #print(W.shape, dW.shape, np.subtract(W , dW).shape)\n",
    "        layer_stack[i+1][\"W\"] = np.subtract(W , dW).T\n",
    "        \n",
    "    return layer_stack\n",
    "\n",
    "train_data = np.array([0 for _ in range(99)] + [1])    \n",
    "    \n",
    "layer_stack = add_layer(initialize_layer(100, 3, relu))\n",
    "layer_stack = add_layer(initialize_layer(3, 10, relu), layer_stack)\n",
    "layer_stack = add_layer(initialize_layer(10, 3, relu), layer_stack)\n",
    "layer_stack = add_layer(initialize_layer(3, 100, softmax), layer_stack)\n",
    "\n",
    "#layer_stack = add_layer(initialize_layer(5, 10, relu))\n",
    "#layer_stack = add_layer(initialize_layer(10, 10, relu), layer_stack)\n",
    "#layer_stack = add_layer(initialize_layer(10, 5, softmax), layer_stack)\n",
    "\n",
    "\n",
    "for _ in range(100): \n",
    "    out, layer_stack = forward_pass(layer_stack, train_data)\n",
    "    dloss = KL_divergence(out, train_data, d=True)\n",
    "    loss = KL_divergence(out, train_data)\n",
    "    print(\"Error\", loss)\n",
    "    layer_stack = back_prop(layer_stack, dloss)\n",
    "    \n",
    "    \n",
    "    \n",
    "#for l, y in layer_stack: print(l,)\n",
    "#activate_layer(W1, b1, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07943046772809711"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.random.standard_normal((5,5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b- a (10,) W (10, 5) b (5,)\n",
      "[[ 8.90074212e-19  2.35942377e-31  3.00566436e+00  8.68645379e-36\n",
      "  -1.01907943e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00]\n",
      " [ 1.52457056e-18  4.04135742e-31  5.14827566e+00  1.48786602e-35\n",
      "  -1.74553815e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00]\n",
      " [ 2.77489213e-18  7.35573098e-31  9.37044830e+00  2.70808568e-35\n",
      "  -3.17707833e-07]\n",
      " [ 5.74623255e-20  1.52322104e-32  1.94042768e-01  5.60789009e-37\n",
      "  -6.57907768e-09]\n",
      " [ 1.36701320e-18  3.62370170e-31  4.61622502e+00  1.33410190e-35\n",
      "  -1.56514481e-07]\n",
      " [ 1.61925762e-18  4.29235547e-31  5.46802149e+00  1.58027345e-35\n",
      "  -1.85394893e-07]\n",
      " [ 1.44709652e-19  3.83598792e-32  4.88665594e-01  1.41225719e-36\n",
      "  -1.65683521e-08]]\n",
      "b- a (10,) W (10, 10) b (10,)\n"
     ]
    }
   ],
   "source": [
    "def back_prop(layer_stack, dloss):\n",
    "    #loss of output layer\n",
    "    g = dloss\n",
    "    layer_out = layer_stack[-1]\n",
    "    W = layer_out[\"W\"]\n",
    "    \n",
    "    dzy_out = activate_layer(layer_out, d=True) #dy/dz\n",
    "    g = np.multiply(g, dzy_out)\n",
    "    \n",
    "    dWz_out = layer_out[\"a\"] #dzt/dWt = at-1\n",
    "    dW_out = np.dot(M(dWz_out).T, M(g))\n",
    "    print(dW_out)\n",
    "    \n",
    "    g = W.dot(g.T)\n",
    "    #stochastic gradient update\n",
    "    #layer_stack[-1][\"W\"] = np.subtract(W , dW_out)\n",
    "    layer_stack[-1][\"da\"] = dWz_out\n",
    "    #print(\"output done\")\n",
    "    \n",
    "    for i, layer in enumerate(reversed(layer_stack[:-1])):\n",
    "        #activate_layer() recomputes z to minimize lines of code (and save some memory)\n",
    "        # It is then activated with the derived activation function\n",
    "        da = activate_layer(layer, d=True)\n",
    "        layer_stack[i][\"da\"] = da\n",
    "        W = layer[\"W\"]\n",
    "        b = layer[\"b\"]\n",
    "        \n",
    "        g = np.multiply(g, da)\n",
    "        dW = g.dot(da)\n",
    "        g = W.dot(g.T)\n",
    "        #print(\"b\", i)\n",
    "        #\n",
    "        layer_stack[i][\"b\"] = np.subtract(b , g)\n",
    "        #layer_stack[i][\"W\"] = np.subtract(W , dW)\n",
    "        return W, dW\n",
    "        break\n",
    "W, dW = back_prop(layer_stack,dloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(M(z).T, M(out)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1) and (5,1) not aligned: 1 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3f0bec24fd61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,1) and (5,1) not aligned: 1 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(M(z), M(out).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = layer_stack[-1][\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (10,5) not aligned: 5 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-267-7cf32ee4bd0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlayer_stack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-267-7cf32ee4bd0d>\u001b[0m in \u001b[0;36mback_prop\u001b[1;34m(layer_stack, dloss)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlayer_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_stack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdzy_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivate_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdWz_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdW_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdzy_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWz_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-266-d82ea078601a>\u001b[0m in \u001b[0;36mactivate_layer\u001b[1;34m(layer, d)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"f\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m#logits aka. pre-nonlinearity activations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,) and (10,5) not aligned: 5 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "def back_prop(layer_stack, dloss):\n",
    "    #loss of output layer\n",
    "    g = dloss\n",
    "    layer_out = layer_stack[-1]\n",
    "    dzy_out = activate_layer(layer_out, d=True)\n",
    "    dWz_out = layer_out[\"a\"]\n",
    "    dW_out = np.multiply(np.dot(dzy_out, dWz_out), M(g).T)\n",
    "    for i, layer in enumerate(reversed(layer_stack[1:])):\n",
    "        #activate_layer() recomputes z to minimize lines of code (and save some memory)\n",
    "        # It is then activated with the derived activation function\n",
    "        W = layer[\"W\"]\n",
    "        a = layer[\"a\"]\n",
    "        da = activate_layer(layer, d=True)\n",
    "        g = np.multiply(W, da)\n",
    "        dW = np.dot(g, da)\n",
    "        #W.T.dot(\n",
    "        #update weights\n",
    "        print(i)\n",
    "        layer_stack[i][\"W\"] = np.subtract(W , dW)\n",
    "\n",
    "back_prop(layer_stack, dloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[50, 50]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(5).dot(np.matrix((10,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(input_data, 1-input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20173562, 0.20101412, 0.19914426, 0.1979388 , 0.2001672 ])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activate_layer(layer_stack[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-24.28723753,  -1.50403064,   3.58202782,   7.68577602,\n",
       "         -4.0952484 ],\n",
       "       [ -8.56865646, -14.92830019,  -2.76581367,   5.32833317,\n",
       "          2.11724833],\n",
       "       [ -1.54027289,  -0.82348857, -11.94957658,  -2.18242981,\n",
       "         -0.91329806],\n",
       "       [ -1.92267791,   2.78450506,  -6.66858303,  -7.24853466,\n",
       "         -3.59767197],\n",
       "       [ -8.08615323,   5.19096932,   0.21809782,   2.01987714,\n",
       "        -10.75081245]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.random.normal(1,1,(10,5)), d=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3): print(layer_stack[i][\"a\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': array([[-0.24816425,  0.21988261, -1.48449639, -1.5973357 , -0.74753241,\n",
       "          0.12115882, -0.44619952,  0.63481989, -0.55062677,  1.12433947],\n",
       "        [ 0.96408728,  0.4378771 , -1.30818295,  0.07434215, -1.10239108,\n",
       "         -0.95139944,  0.10283105, -0.08025586, -1.39695337,  0.94091781],\n",
       "        [-0.069817  ,  0.95687134, -0.67180139, -0.05761391,  1.96948677,\n",
       "         -0.25810702, -1.34727975, -0.10348914, -0.27160298, -0.19018819],\n",
       "        [ 0.37815759,  0.39078865,  0.39477496, -1.34882148, -0.64791109,\n",
       "          0.66595316, -0.37375519,  0.23456121,  0.26838649,  2.0526692 ],\n",
       "        [ 0.02667162,  1.34457704, -0.82727016,  0.29429427,  0.13759172,\n",
       "          0.69230056, -0.93676542, -0.55213469, -0.54194377,  0.6625922 ],\n",
       "        [-1.67791053,  0.73742752,  0.80887769,  0.89761816,  1.14922293,\n",
       "         -1.28356169,  0.84502726,  0.93986819, -0.25530766,  0.89848511],\n",
       "        [ 0.28397142,  1.36500138,  2.00430181,  0.29615313,  0.3974948 ,\n",
       "          0.6702975 ,  0.03091867,  2.50085138, -0.8838889 , -0.32561624],\n",
       "        [ 0.44860211, -0.68943808,  1.36551092,  1.49151404, -1.33943659,\n",
       "         -0.89318332, -0.92171081,  0.54553575,  0.47514703,  0.17621213],\n",
       "        [ 0.24096143,  0.87560764, -0.21059471,  0.36174145,  0.54419218,\n",
       "          0.5048296 ,  1.07383778, -2.24924353, -0.05759224, -0.09009494],\n",
       "        [-1.25745341,  0.80811698,  0.25599951,  1.87334336, -1.24972389,\n",
       "          0.84675664, -0.05870703,  0.18869571, -0.20060032, -0.65382523]]),\n",
       " 'a': array([0.001, 0.001, 0.001, 0.001, 0.001]),\n",
       " 'b': array([0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "        0.001]),\n",
       " 'f': <function __main__.relu>}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_stack[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'W': array([[-0.24816425,  0.21988261, -1.48449639, -1.5973357 , -0.74753241,\n",
       "           0.12115882, -0.44619952,  0.63481989, -0.55062677,  1.12433947],\n",
       "         [ 0.96408728,  0.4378771 , -1.30818295,  0.07434215, -1.10239108,\n",
       "          -0.95139944,  0.10283105, -0.08025586, -1.39695337,  0.94091781],\n",
       "         [-0.069817  ,  0.95687134, -0.67180139, -0.05761391,  1.96948677,\n",
       "          -0.25810702, -1.34727975, -0.10348914, -0.27160298, -0.19018819],\n",
       "         [ 0.37815759,  0.39078865,  0.39477496, -1.34882148, -0.64791109,\n",
       "           0.66595316, -0.37375519,  0.23456121,  0.26838649,  2.0526692 ],\n",
       "         [ 0.02667162,  1.34457704, -0.82727016,  0.29429427,  0.13759172,\n",
       "           0.69230056, -0.93676542, -0.55213469, -0.54194377,  0.6625922 ],\n",
       "         [-1.67791053,  0.73742752,  0.80887769,  0.89761816,  1.14922293,\n",
       "          -1.28356169,  0.84502726,  0.93986819, -0.25530766,  0.89848511],\n",
       "         [ 0.28397142,  1.36500138,  2.00430181,  0.29615313,  0.3974948 ,\n",
       "           0.6702975 ,  0.03091867,  2.50085138, -0.8838889 , -0.32561624],\n",
       "         [ 0.44860211, -0.68943808,  1.36551092,  1.49151404, -1.33943659,\n",
       "          -0.89318332, -0.92171081,  0.54553575,  0.47514703,  0.17621213],\n",
       "         [ 0.24096143,  0.87560764, -0.21059471,  0.36174145,  0.54419218,\n",
       "           0.5048296 ,  1.07383778, -2.24924353, -0.05759224, -0.09009494],\n",
       "         [-1.25745341,  0.80811698,  0.25599951,  1.87334336, -1.24972389,\n",
       "           0.84675664, -0.05870703,  0.18869571, -0.20060032, -0.65382523]]),\n",
       "  'a': array([0.001, 0.001, 0.001, 0.001, 0.001]),\n",
       "  'b': array([0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "         0.001]),\n",
       "  'f': <function __main__.relu>},\n",
       " {'W': array([[ 0.58848306,  0.69741806, -0.44532863, -1.11673609, -0.8423204 ,\n",
       "           1.08287802, -2.2591627 , -0.49785627,  0.31520698, -0.1625122 ],\n",
       "         [-0.82425657,  0.35517525, -0.50495707, -0.60094305,  1.58914961,\n",
       "           1.14451094,  0.15824603,  1.89779666,  0.26035949, -0.34300505],\n",
       "         [-0.59390702,  2.0470155 , -0.76928132,  0.8188989 , -1.06204832,\n",
       "          -0.32997477,  1.7311974 , -0.65800514,  0.02038337, -0.09869588],\n",
       "         [ 0.46640205,  0.84546074, -0.51813214,  1.50931264,  0.7620151 ,\n",
       "          -0.50226775,  1.2539965 , -1.27197667, -0.73048443, -0.58726613],\n",
       "         [-0.45534928,  0.17413576,  0.41612679, -0.63296525, -0.61442337,\n",
       "           2.19005376, -0.66693279,  0.87778483, -0.55204795, -0.19198862]]),\n",
       "  'a': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'b': array([0.001, 0.001, 0.001, 0.001, 0.001]),\n",
       "  'f': <function __main__.relu>}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(layer_stack[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in reversed(layer_stack[:-1]): print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_prop(layer_stack, dloss, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8000000000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(out, d=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': array([[-1.36499269, -0.7921027 ,  1.65143855, -0.13963861,  0.56097456],\n",
       "        [-0.55209666,  0.44693978,  0.71740411,  0.4495377 , -1.11230445],\n",
       "        [ 1.72761561, -0.34032056,  0.89761843,  1.05949191, -0.00922064],\n",
       "        [ 1.86546767,  0.07417857, -0.55126836, -0.83688171,  0.40181989],\n",
       "        [-0.92705267,  0.36303249, -0.03409175, -1.58309083,  0.86056945]]),\n",
       " 'a': array([0., 0., 0., 0., 0.]),\n",
       " 'b': array([0.001, 0.001, 0.001, 0.001, 0.001]),\n",
       " 'f': <function __main__.softmax>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_stack[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.734723475976807e-17"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(dloss,out).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-3f786850e387>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorizer(tokenized_text):\n",
    "    words = set(tokenized_text)\n",
    "    vocab_size = len(words)\n",
    "    onehot = np.identity(vocab_size)\n",
    "    \n",
    "    word2onehot = dict(zip(words, onehot))\n",
    "    onehotargmax2word = dict(zip(onehot.argmax(axis=1), words)) \n",
    "                                #cant use array as key\n",
    "                                #onehot.argmax(axis=1) same as range(vocab_size)\n",
    "    \n",
    "    return word2onehot, onehotargmax2word\n",
    "    \n",
    "    \n",
    "tokenized_text = \"this is a test to se how well it all works -did i say it was a test?\".split()\n",
    "\n",
    "w2o, o2w = one_hot_vectorizer(tokenized_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
