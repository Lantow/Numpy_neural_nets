{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.9678056 , -1.73288255,  0.73990348],\n",
       "        [-0.36857356, -0.2094218 , -1.25618375],\n",
       "        [ 3.75493236,  0.31782993, -0.11016211]]),\n",
       " array([[0.001, 0.001, 0.001],\n",
       "        [0.001, 0.001, 0.001],\n",
       "        [0.001, 0.001, 0.001]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_layer(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    x[x < 0] = 0   \n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    sm = lambda z: np.exp(z)/np.sum(np.exp(z))\n",
    "    return sm(x) #np.apply_along_axis(sm, arr=x, axis=1)\n",
    "\n",
    "# Shape(3, 3)\n",
    "# 3 input neurons and one layer with 10 hidden neurons and an output of three neurons\n",
    "\n",
    "def rnn(shape, activation):\n",
    "    # [3, 10, 3 ,1]\n",
    "    # return layer_stack\n",
    "    pass\n",
    "\n",
    "def initialize_layer(this_layer_neurons, previous_layer_neurons, \n",
    "                     activation=relu): \n",
    "    size = (this_layer_neurons, previous_layer_neurons)\n",
    "    weights = np.random.standard_normal(size)\n",
    "    # http://cs231n.github.io/neural-networks-2/ initialized at 0.001\n",
    "    biases = np.full(previous_layer_neurons, 0.01)\n",
    "    return weights, biases, activation\n",
    "\n",
    "def activate_layer(previous_layer_weights, this_layer_weiths, \n",
    "                   bias, activation_function):\n",
    "    z = np.dot(this_layer_weiths, previous_layer_weights)+ bias\n",
    "    return activation_function(z)\n",
    "\n",
    "def add_layer(layer, L=[]):\n",
    "    return L + [layer]\n",
    "\n",
    "def forward_pass(layer_stack, input_data):\n",
    "    weights0 = input_data\n",
    "    while layer_stack:\n",
    "        weights1, bias, activation = layer_stack.pop(0)\n",
    "        weights0 = activate_layer(weights1, weights0, \n",
    "                                  bias, activation)\n",
    "        \n",
    "    return weights0\n",
    "\n",
    "input_data = np.array([0 for _ in range(99)] + [1])\n",
    "\n",
    "layer_stack = add_layer(initialize_layer(100, 3, relu))\n",
    "layer_stack = add_layer(initialize_layer(3, 10, relu), layer_stack)\n",
    "layer_stack = add_layer(initialize_layer(10, 3, relu), layer_stack)\n",
    "layer_stack = add_layer(initialize_layer(3, 100, softmax), layer_stack)\n",
    "\n",
    "d = forward_pass(layer_stack, input_data)\n",
    "\n",
    "#for l, y in layer_stack: print(l,)\n",
    "#activate_layer(W1, b1, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
